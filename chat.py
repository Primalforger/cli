"""Chat engine â€” streaming, tool use, context management."""

import re
import json
import os
import subprocess

import httpx
from rich.console import Console

from tools import TOOL_MAP, TOOL_DESCRIPTIONS, validate_file_references, is_tool_read_only
from context_manager import (
    ContextBudget, smart_compact, condense_file_contents,
    estimate_message_tokens,
)

from metrics import MetricsTracker

console = Console()
tracker = MetricsTracker()


# â”€â”€ Display helpers (safe imports) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _show_thinking() -> bool:
    try:
        from display import show_thinking
        return show_thinking()
    except (ImportError, AttributeError):
        return True


def _show_metrics() -> bool:
    try:
        from display import show_metrics
        return show_metrics()
    except (ImportError, AttributeError):
        return False


def _show_tool_output() -> bool:
    try:
        from display import show_tool_output
        return show_tool_output()
    except (ImportError, AttributeError):
        return True


def _show_streaming() -> bool:
    try:
        from display import show_streaming
        return show_streaming()
    except (ImportError, AttributeError):
        return True


def _get_verbosity():
    try:
        from display import get_verbosity, Verbosity
        return get_verbosity(), Verbosity
    except (ImportError, AttributeError):
        return 1, None


# â”€â”€ Error Diagnosis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def diagnose_test_error(error_output: str) -> dict:
    """
    Parse test/build error output and produce a structured diagnosis.
    Returns dict with error type, root cause, affected files, and fix guidance.
    """
    diagnosis = {
        "error_type": "unknown",
        "root_cause": "",
        "affected_files": [],
        "missing_module": "",
        "import_chain": [],
        "fix_guidance": "",
        "is_local_import": False,
        "is_pip_package": False,
    }

    # â”€â”€ Extract the actual error line (last line of traceback) â”€â”€
    lines = error_output.strip().split("\n")
    error_line = ""
    for line in reversed(lines):
        stripped = line.strip()
        if stripped.startswith(("ModuleNotFoundError:", "ImportError:",
                                "SyntaxError:", "IndentationError:",
                                "NameError:", "AttributeError:")):
            error_line = stripped
            break

    if not error_line:
        return diagnosis

    # â”€â”€ ModuleNotFoundError â”€â”€
    mod_match = re.search(
        r"ModuleNotFoundError: No module named ['\"](.+?)['\"]",
        error_line,
    )
    if mod_match:
        missing = mod_match.group(1)
        diagnosis["error_type"] = "missing_module"
        diagnosis["missing_module"] = missing

        # Extract the import chain from traceback
        file_pattern = re.compile(
            r'(?:File "(.+?)".*line (\d+))|(?:^(\S+\.py):(\d+):)',
            re.MULTILINE,
        )
        for m in file_pattern.finditer(error_output):
            fpath = m.group(1) or m.group(3)
            lineno = m.group(2) or m.group(4)
            if fpath and not fpath.startswith(("C:\\Python", "/usr/lib",
                                               "<", "importlib")):
                diagnosis["import_chain"].append(f"{fpath}:{lineno}")
                if fpath not in diagnosis["affected_files"]:
                    diagnosis["affected_files"].append(fpath)

        # â”€â”€ Key decision: is this a LOCAL module or a pip package? â”€â”€
        top_level = missing.split(".")[0]

        # Check if it looks like a local module
        from pathlib import Path
        cwd = Path.cwd()
        local_indicators = [
            (cwd / f"{top_level}.py").is_file(),
            (cwd / top_level).is_dir(),
            (cwd / "src" / f"{top_level}.py").is_file(),
            (cwd / "src" / top_level).is_dir(),
            (cwd / "lib" / f"{top_level}.py").is_file(),
            (cwd / "app" / f"{top_level}.py").is_file(),
            top_level in ("models", "crawler", "app", "config",
                          "utils", "helpers", "views", "routes",
                          "schemas", "services", "database", "db",
                          "api", "core", "common", "settings",
                          "urls", "forms", "serializers", "tasks",
                          "middleware", "decorators", "exceptions",
                          "constants", "enums", "managers"),
        ]

        if any(local_indicators):
            diagnosis["is_local_import"] = True
            diagnosis["is_pip_package"] = False
            diagnosis["root_cause"] = (
                f"Module '{missing}' exists as a local file but Python "
                f"can't find it. This is an IMPORT PATH issue, not a "
                f"missing pip package."
            )

            # Figure out the specific fix needed
            src_file = cwd / "src" / f"{top_level}.py"
            src_dir = cwd / "src" / top_level
            bare_file = cwd / f"{top_level}.py"
            bare_dir = cwd / top_level

            if (src_file.is_file() or src_dir.is_dir()) and not bare_file.is_file() and not bare_dir.is_dir():
                diagnosis["fix_guidance"] = (
                    f"The file exists at 'src/{top_level}.py' but is being "
                    f"imported as '{missing}' (without the 'src.' prefix). "
                    f"FIXES (choose one):\n"
                    f"  1. Change the import in the IMPORTING file to "
                    f"'from src.{missing} import ...' â€” this is the best fix\n"
                    f"  2. Add a conftest.py that adds 'src' to sys.path\n"
                    f"  3. Add 'src' to sys.path at the top of the importing file\n"
                    f"DO NOT add '{missing}' to requirements.txt â€” "
                    f"it is NOT a pip package.\n"
                    f"DO NOT modify requirements.txt at all for this error."
                )
            elif bare_file.is_file() or bare_dir.is_dir():
                diagnosis["fix_guidance"] = (
                    f"The file '{top_level}.py' exists in the project root "
                    f"but Python can't find it. The importing file may be "
                    f"running from a different directory. Check:\n"
                    f"  1. Is there a sys.path issue?\n"
                    f"  2. Does conftest.py add the project root to sys.path?\n"
                    f"  3. Does the project need a setup.py or pyproject.toml "
                    f"with package configuration?\n"
                    f"DO NOT add '{missing}' to requirements.txt."
                )
            else:
                diagnosis["fix_guidance"] = (
                    f"Module '{missing}' looks like a local module name but "
                    f"the file wasn't found. Check:\n"
                    f"  1. Does the file need to be created?\n"
                    f"  2. Is the import path/spelling wrong?\n"
                    f"  3. Search the project for files that might match.\n"
                    f"DO NOT add '{missing}' to requirements.txt unless you "
                    f"are CERTAIN it's a third-party package listed on PyPI."
                )
        else:
            diagnosis["is_local_import"] = False
            diagnosis["is_pip_package"] = True
            diagnosis["root_cause"] = (
                f"Module '{missing}' appears to be a third-party package "
                f"that's not installed."
            )
            diagnosis["fix_guidance"] = (
                f"Add '{missing}' to requirements.txt with an appropriate "
                f"version pin, then reinstall dependencies."
            )

        return diagnosis

    # â”€â”€ ImportError: cannot import name â”€â”€
    name_match = re.search(
        r"ImportError: cannot import name ['\"](.+?)['\"] from ['\"](.+?)['\"]",
        error_line,
    )
    if name_match:
        symbol = name_match.group(1)
        module = name_match.group(2)
        diagnosis["error_type"] = "missing_symbol"
        diagnosis["missing_module"] = module
        diagnosis["root_cause"] = (
            f"Module '{module}' exists but doesn't export '{symbol}'. "
            f"This could be a version mismatch (the symbol was added in "
            f"a newer version or removed in the current one), or the "
            f"symbol name is misspelled, or it's defined in a different "
            f"submodule."
        )
        diagnosis["fix_guidance"] = (
            f"1. Use read_file to check the actual source of '{module}' "
            f"and see what it exports\n"
            f"2. Check what version of '{module}' provides '{symbol}'\n"
            f"3. Update the version in requirements.txt if needed\n"
            f"4. Or fix the import if the symbol name is wrong\n"
            f"5. If '{module}' is a local file, read it and check what "
            f"names are defined in it"
        )

        # Check if the module is local
        from pathlib import Path
        cwd = Path.cwd()
        top_level = module.split(".")[0]
        if (cwd / f"{top_level}.py").is_file() or (cwd / "src" / f"{top_level}.py").is_file():
            diagnosis["is_local_import"] = True
            diagnosis["fix_guidance"] += (
                f"\n\nThis appears to be a LOCAL module. Read the file "
                f"to check what's actually defined in it before changing "
                f"requirements.txt."
            )

        return diagnosis

    # â”€â”€ SyntaxError â”€â”€
    if "SyntaxError" in error_line:
        diagnosis["error_type"] = "syntax_error"
        for line in lines:
            if 'File "' in line and '.py"' in line:
                fmatch = re.search(r'File "(.+?)".*line (\d+)', line)
                if fmatch:
                    diagnosis["affected_files"].append(fmatch.group(1))
        diagnosis["root_cause"] = f"Syntax error in source file: {error_line}"
        diagnosis["fix_guidance"] = (
            "Read the file mentioned in the traceback and fix the syntax "
            "error. Use read_file to see the actual file content.\n"
            "Do NOT modify requirements.txt for syntax errors."
        )
        return diagnosis

    # â”€â”€ IndentationError â”€â”€
    if "IndentationError" in error_line:
        diagnosis["error_type"] = "indentation_error"
        for line in lines:
            if 'File "' in line and '.py"' in line:
                fmatch = re.search(r'File "(.+?)".*line (\d+)', line)
                if fmatch:
                    diagnosis["affected_files"].append(fmatch.group(1))
        diagnosis["root_cause"] = f"Indentation error: {error_line}"
        diagnosis["fix_guidance"] = (
            "Read the file and fix the indentation. "
            "Do NOT modify requirements.txt for indentation errors."
        )
        return diagnosis

    # â”€â”€ AttributeError â”€â”€
    attr_match = re.search(
        r"AttributeError: (?:module |type object )?['\"]?(.+?)['\"]? has no attribute ['\"](.+?)['\"]",
        error_line,
    )
    # â”€â”€ Shared file state (accumulated tasks.json / data file between tests) â”€â”€
    additional_match = re.search(
        r'First list contains (\d+) additional elements', error_output
    )
    if additional_match or "First extra element" in error_output:
        count = additional_match.group(1) if additional_match else "multiple"
        diagnosis["error_type"] = "shared_file_state"
        diagnosis["root_cause"] = (
            f"Tests are loading stale data from a persistent file (e.g. tasks.json). "
            f"The list has {count} extra elements left over from previous test runs. "
            f"Each test instantiates the class and it loads ALL prior data from disk."
        )
        diagnosis["fix_guidance"] = (
            "1. Add data_file=None support to the class __init__:\n"
            "     def __init__(self, data_file='tasks.json'):\n"
            "         self.data_file = data_file\n"
            "         self.tasks = []\n"
            "         if data_file: self.load_tasks()\n"
            "2. In every test, use setUp():\n"
            "     def setUp(self):\n"
            "         self.manager = TodoManager(data_file=None)\n"
            "3. In tearDown(), delete any leftover data files:\n"
            "     def tearDown(self):\n"
            "         if os.path.exists('tasks.json'): os.remove('tasks.json')\n"
            "4. Delete the existing tasks.json from the project root right now.\n"
            "DO NOT change test assertions â€” fix the class and test setup instead."
        )
        return diagnosis

    # â”€â”€ ConnectionRefusedError â”€â”€
    if "ConnectionRefusedError" in error_output or "Connection refused" in error_output:
        diagnosis["error_type"] = "connection_refused"
        diagnosis["root_cause"] = (
            "Test is connecting to a real server that isn't running. "
            "Unit/integration tests must use the framework's test client, not real HTTP."
        )
        diagnosis["fix_guidance"] = (
            "Replace all requests.get/post('http://localhost:...') with:\n"
            "  Flask: client = app.test_client(); client.get('/route')\n"
            "  FastAPI: client = TestClient(app); client.get('/route')\n"
            "Remove any app.run() or server.listen() calls from test files."
        )
        return diagnosis

    # â”€â”€ IntegrityError / UniqueViolation â”€â”€
    if any(p in error_output for p in (
        "IntegrityError", "UniqueViolation", "UNIQUE constraint failed",
        "duplicate key value violates", "NOT NULL constraint failed",
    )):
        diagnosis["error_type"] = "db_integrity_error"
        diagnosis["root_cause"] = (
            "Tests are sharing database state. A prior test left rows that violate "
            "a UNIQUE or NOT NULL constraint in the next test."
        )
        diagnosis["fix_guidance"] = (
            "Add to setUp():  db.drop_all(); db.create_all()\n"
            "Add to tearDown(): db.session.remove(); db.drop_all()\n"
            "Use 'sqlite:///:memory:' as the test DB URI.\n"
            "For Django, use django.test.TestCase (auto-rollback per test)."
        )
        return diagnosis

    # â”€â”€ OperationalError: no such table â”€â”€
    if any(p in error_output for p in (
        "no such table", "relation does not exist", "Table doesn't exist",
    )):
        diagnosis["error_type"] = "db_table_missing"
        diagnosis["root_cause"] = (
            "Test database tables were never created. "
            "db.create_all() must run inside the app context before any test."
        )
        diagnosis["fix_guidance"] = (
            "In setUp(), after setting the test DB URI:\n"
            "  with app.app_context():\n"
            "      db.create_all()\n"
            "For FastAPI/SQLModel: SQLModel.metadata.create_all(engine)\n"
            "Ensure this runs BEFORE any test method that touches the DB."
        )
        return diagnosis

    # â”€â”€ Missing env var (KeyError on os.environ) â”€â”€
    env_key_match = re.search(
        r"KeyError: ['\"]([A-Z_]{3,})['\"]", error_output
    )
    if env_key_match and any(p in error_output for p in (
        "os.environ", "os.getenv", "environ[", "getenv("
    )):
        missing_key = env_key_match.group(1)
        diagnosis["error_type"] = "missing_env_var"
        diagnosis["missing_module"] = missing_key
        diagnosis["root_cause"] = (
            f"Environment variable '{missing_key}' is not set in the test environment."
        )
        diagnosis["fix_guidance"] = (
            f"In setUp() or conftest.py, set a safe test default:\n"
            f"  os.environ['{missing_key}'] = 'test_value'\n"
            f"Or use: @unittest.mock.patch.dict(os.environ, {{'{missing_key}': 'test'}})\n"
            f"Never rely on a real .env file being present during automated tests."
        )
        return diagnosis

    # â”€â”€ AttributeError â”€â”€
    attr_match = re.search(
        r"AttributeError: (?:module |type object )?['\"]?(.+?)['\"]? has no attribute ['\"](.+?)['\"]",
        error_line,
    )
    if attr_match:
        obj = attr_match.group(1)
        attr = attr_match.group(2)
        diagnosis["error_type"] = "attribute_error"
        diagnosis["root_cause"] = (
            f"'{obj}' doesn't have attribute '{attr}'. This is usually "
            f"a version mismatch or API change."
        )
        diagnosis["fix_guidance"] = (
            f"1. Check which version of the package provides '{attr}'\n"
            f"2. Read the source file to see what's available\n"
            f"3. Update the version in requirements.txt if it's a "
            f"third-party package version issue"
        )
        return diagnosis

    return diagnosis


def format_error_guidance(result_text: str) -> str:
    """
    Analyze test failure output and append smart, specific guidance
    so the LLM knows exactly what to fix instead of guessing.
    """
    diagnosis = diagnose_test_error(result_text)

    if diagnosis["error_type"] == "unknown":
        return (
            "\n\n" + "=" * 60 + "\n"
            "âš  IMPORTANT: The output above contains the FULL error traceback.\n"
            "Read the LAST line of each traceback first â€” it has the actual error.\n"
            "Then trace back through the file paths to find which file to fix.\n"
            "Do NOT guess. Do NOT add random packages to requirements.txt.\n"
            "Use read_file to examine the files mentioned in the traceback.\n"
            + "=" * 60
        )

    parts = ["\n\n" + "=" * 60]
    parts.append("âš  ERROR DIAGNOSIS (auto-generated â€” read carefully)")
    parts.append("=" * 60)
    parts.append(f"\nError type: {diagnosis['error_type']}")

    if diagnosis["missing_module"]:
        parts.append(f"Missing module: {diagnosis['missing_module']}")

    parts.append(f"\nRoot cause: {diagnosis['root_cause']}")

    if diagnosis["affected_files"]:
        parts.append("\nAffected files (read these with read_file):")
        for f in diagnosis["affected_files"]:
            parts.append(f"  â†’ {f}")

    if diagnosis["import_chain"]:
        parts.append("\nImport chain (how we got to the error):")
        for step in diagnosis["import_chain"]:
            parts.append(f"  â†’ {step}")

    parts.append(f"\nðŸ”§ HOW TO FIX:\n{diagnosis['fix_guidance']}")

    if diagnosis["is_local_import"]:
        parts.append(
            "\n" + "!" * 60 + "\n"
            "ðŸš« CRITICAL: This is a LOCAL module, NOT a pip package.\n"
            "Do NOT modify requirements.txt.\n"
            "Do NOT add this module name to requirements.txt.\n"
            "Fix the IMPORT PATH in the Python source file instead.\n"
            "Use read_file to examine the affected files listed above.\n"
            + "!" * 60
        )

    if diagnosis["error_type"] == "syntax_error":
        parts.append(
            "\nðŸš« CRITICAL: This is a syntax error in YOUR code.\n"
            "Do NOT modify requirements.txt. Read and fix the file."
        )

    if diagnosis["error_type"] == "indentation_error":
        parts.append(
            "\nðŸš« CRITICAL: This is an indentation error in YOUR code.\n"
            "Do NOT modify requirements.txt. Read and fix the file."
        )

    parts.append("=" * 60)

    return "\n".join(parts)


# â”€â”€ Test Failure Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _is_test_failure(result_text: str) -> bool:
    """Detect if tool results contain a test/build failure."""
    failure_indicators = [
        "FAILED", "ERRORS", "exit 2", "exit 1",
        "ImportError", "ModuleNotFoundError",
        "SyntaxError", "IndentationError",
        "cannot import name", "No module named",
        "error during collection", "collection error",
        "ModuleNotFoundError:", "ImportError:",
        "FAILED (errors=", "ERROR collecting",
    ]
    return any(indicator in result_text for indicator in failure_indicators)


# â”€â”€ Tool Call Parsing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_tool_calls(text: str) -> list[tuple[str, str]]:
    """Parse tool calls â€” robust against LLM formatting quirks.

    Handles:
    - Properly closed: <tool:name>args</tool>
    - Unclosed tags:   <tool:name>args (no closing tag)
    - Backtick-wrapped args: <tool:name>`path`</tool>
    - Extra whitespace/newlines in args
    - Multiple tool calls in one response
    """
    results = []

    # 1. Match properly closed tags first (most reliable)
    closed_pattern = r"<tool:(\w+)>(.*?)</tool>"
    for match in re.finditer(closed_pattern, text, re.DOTALL):
        tool_name = match.group(1)
        tool_args = match.group(2).strip()
        tool_args = _clean_tool_args(tool_args)
        if tool_name and tool_args is not None:
            results.append((tool_name, tool_args))

    if results:
        return results

    # 2. Fallback: unclosed tags â€” grab to end of line
    unclosed_pattern = r"<tool:(\w+)>\s*([^\n<]+)"
    for match in re.finditer(unclosed_pattern, text):
        tool_name = match.group(1)
        tool_args = match.group(2).strip()
        tool_args = _clean_tool_args(tool_args)

        if not tool_args:
            continue
        if len(tool_args) > 500:
            continue
        if '<tool:' in tool_args:
            continue

        results.append((tool_name, tool_args))

    # 3. Last resort: unclosed multi-line (for write_file, edit_file, run_python)
    if not results:
        multiline_pattern = r"<tool:(\w+)>\s*(.+?)(?=<tool:|\Z)"
        for match in re.finditer(multiline_pattern, text, re.DOTALL):
            tool_name = match.group(1)
            tool_args = match.group(2).strip()

            multiline_tools = {"write_file", "edit_file", "run_python"}
            if tool_name in multiline_tools:
                tool_args = _clean_tool_args(tool_args)
                if tool_args:
                    results.append((tool_name, tool_args))
            else:
                first_line = tool_args.split("\n")[0].strip()
                first_line = _clean_tool_args(first_line)
                if first_line and len(first_line) < 500:
                    results.append((tool_name, first_line))

    return results


def _clean_tool_args(args: str) -> str:
    """Clean tool arguments from LLM formatting artifacts."""
    if not args:
        return args

    cleaned = args

    if cleaned.rstrip().endswith('</tool>'):
        cleaned = cleaned.rstrip()
        cleaned = cleaned[:-7].rstrip()

    if cleaned.startswith('`') and cleaned.endswith('`') and '\n' not in cleaned:
        cleaned = cleaned[1:-1]

    if len(cleaned) >= 2 and cleaned[0] in ('"', "'") and cleaned[-1] == cleaned[0]:
        if '\n' not in cleaned:
            cleaned = cleaned[1:-1]

    cleaned = cleaned.strip('*_')
    cleaned = cleaned.strip()

    return cleaned


# â”€â”€ Hallucination Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_FILE_QUERY_KEYWORDS = [
    "file structure", "file tree", "directory", "show me the file",
    "list files", "what files", "project structure", "folder structure",
    "show me the structure", "show the structure", "show me the project",
    "what's in", "whats in", "contents of", "show files",
    "show folders", "show directories", "tree", "ls", "dir",
    "file listing", "project files", "source files", "code files",
    "show me the fie",
]

_FAKE_TREE_INDICATORS = [
    "â”œâ”€â”€",
    "â””â”€â”€",
    "â”‚   ",
    "```plaintext",
    "```text",
    "```\napp.py",
]


def detect_hallucinated_files(user_input: str, response: str) -> bool:
    """Detect if the model faked file operations instead of using tools."""
    user_lower = user_input.lower()

    asked_about_files = any(kw in user_lower for kw in _FILE_QUERY_KEYWORDS)
    if not asked_about_files:
        return False

    tool_calls = parse_tool_calls(response)
    if tool_calls:
        return False

    has_fake_tree = any(indicator in response for indicator in _FAKE_TREE_INDICATORS)
    if has_fake_tree:
        return True

    path_lines = 0
    for line in response.split("\n"):
        stripped = line.strip().strip("- ")
        if re.match(r'^[\w./\\][\w./\\-]+\.\w+$', stripped):
            path_lines += 1

    if path_lines >= 3:
        return True

    return False


def detect_hallucinated_content(user_input: str, response: str) -> bool:
    """Detect if the model faked reading a file."""
    user_lower = user_input.lower()

    read_keywords = [
        "read ", "show me ", "open ", "cat ", "display ",
        "what's in ", "whats in ", "contents of ",
        "show the code", "show the file", "look at ",
    ]

    wants_to_read = any(kw in user_lower for kw in read_keywords)
    has_filename = bool(re.search(r'\b\w+\.\w{1,5}\b', user_lower))

    if not (wants_to_read and has_filename):
        return False

    tool_calls = parse_tool_calls(response)
    if any(name == "read_file" for name, _ in tool_calls):
        return False

    code_block_match = re.search(r'```\w*\n(.+?)```', response, re.DOTALL)
    if code_block_match:
        code_content = code_block_match.group(1)
        if len(code_content.split('\n')) > 5:
            return True

    return False


# â”€â”€ Import Reference Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def validate_import_reference(import_str: str, base_dir: str | None = None) -> bool:
    """
    Check if a dotted import resolves to an actual file/package.

    Walks the dotted path RIGHT to LEFT, peeling off segments
    that might be symbols (functions, classes, variables) until
    we find a .py file or package that exists.

    Examples:
        'src.crawler.fetch_character_info' -> finds src/crawler.py
        'src.models.db'                   -> finds src/models.py
        'src.models.Character'            -> finds src/models.py
        'src.app'                         -> finds src/app.py
    """
    if not import_str:
        return False

    from pathlib import Path
    base = Path(base_dir).resolve() if base_dir else Path.cwd().resolve()
    parts = import_str.split(".")

    for i in range(len(parts), 0, -1):
        candidate = parts[:i]
        module_path = "/".join(candidate)

        # Check as .py file: src/crawler.py
        py_file = base / (module_path + ".py")
        if py_file.is_file():
            return True

        # Check as package: src/crawler/__init__.py
        pkg_init = base / module_path / "__init__.py"
        if pkg_init.is_file():
            return True

        # Check as namespace package directory
        pkg_dir = base / module_path
        if pkg_dir.is_dir():
            return True

    return False


# Known stdlib / common third-party top-level modules
_EXTERNAL_MODULES = {
    "os", "sys", "re", "json", "math", "time", "datetime", "pathlib",
    "collections", "functools", "itertools", "typing", "dataclasses",
    "abc", "io", "logging", "unittest", "subprocess", "shutil",
    "argparse", "copy", "hashlib", "hmac", "secrets", "random",
    "string", "textwrap", "struct", "enum", "socket", "http",
    "urllib", "email", "html", "xml", "csv", "sqlite3", "ast",
    "inspect", "importlib", "contextlib", "concurrent", "threading",
    "multiprocessing", "asyncio", "signal", "tempfile", "glob",
    "fnmatch", "stat", "platform", "traceback", "warnings",
    "pprint", "pickle", "shelve", "marshal", "base64", "binascii",
    "codecs", "locale", "gettext", "unicodedata", "decimal",
    "fractions", "operator", "array", "heapq", "bisect",
    "queue", "types", "weakref", "gc", "dis", "token",
    "tokenize", "pdb", "profile", "timeit", "cProfile",
    "configparser", "tomllib", "zipfile", "tarfile", "gzip",
    "bz2", "lzma", "zlib", "uuid",
    # Common third-party
    "flask", "django", "fastapi", "requests", "httpx", "aiohttp",
    "sqlalchemy", "pydantic", "celery", "redis", "pymongo",
    "psycopg2", "mysql", "boto3", "botocore", "numpy", "pandas",
    "scipy", "matplotlib", "sklearn", "torch", "tensorflow",
    "pytest", "nose", "mock", "faker", "factory",
    "rich", "click", "typer", "fire", "prompt_toolkit",
    "yaml", "toml", "dotenv", "decouple", "environ",
    "PIL", "cv2", "jinja2", "mako", "markupsafe",
    "werkzeug", "gunicorn", "uvicorn", "starlette",
    "marshmallow", "wtforms", "babel", "alembic",
    "setuptools", "pkg_resources", "pip", "wheel",
    "bs4", "beautifulsoup4", "scrapy", "selenium", "lxml",
    "cryptography", "jwt", "passlib", "bcrypt",
}


def _is_likely_external(module: str) -> bool:
    """Check if a module is likely stdlib or third-party."""
    top_level = module.split(".")[0]
    return top_level in _EXTERNAL_MODULES


def check_file_imports(filepath: str, base_dir: str | None = None) -> list[dict]:
    """
    Parse a Python file's imports and validate each one.
    Returns list of broken import references.
    Only checks local imports (skips stdlib/third-party).
    """
    from pathlib import Path

    path = Path(filepath)
    if not path.is_file() or path.suffix != ".py":
        return []

    base = base_dir or str(Path.cwd())

    try:
        content = path.read_text(encoding="utf-8")
    except (UnicodeDecodeError, PermissionError):
        return []

    broken = []

    import_patterns = [
        (r'^from\s+([\w.]+)\s+import\s+(.+?)(?:#.*)?$', 'from'),
        (r'^import\s+([\w.]+(?:\s*,\s*[\w.]+)*)(?:#.*)?$', 'import'),
    ]

    for line_num, line in enumerate(content.split("\n"), 1):
        stripped = line.strip()
        if not stripped or stripped.startswith("#"):
            continue

        for pattern, import_type in import_patterns:
            match = re.match(pattern, stripped)
            if not match:
                continue

            if import_type == 'from':
                module = match.group(1)
                symbols = match.group(2)

                if module.startswith('.'):
                    continue
                if _is_likely_external(module):
                    continue

                if not validate_import_reference(module, base):
                    for sym in re.split(r'\s*,\s*', symbols):
                        sym = sym.strip().split(' as ')[0].strip()
                        sym = sym.strip('()')
                        if sym and sym not in ('', '(', ')'):
                            broken.append({
                                "file": str(filepath),
                                "line": line_num,
                                "module": module,
                                "symbol": sym,
                                "full_import": f"{module}.{sym}",
                                "message": (
                                    f"`{filepath}` imports `{module}.{sym}` "
                                    f"but module `{module}` not found"
                                ),
                            })

            elif import_type == 'import':
                modules_str = match.group(1)
                for mod in re.split(r'\s*,\s*', modules_str):
                    mod = mod.strip().split(' as ')[0].strip()
                    if not mod or mod.startswith('.'):
                        continue
                    if _is_likely_external(mod):
                        continue
                    if not validate_import_reference(mod, base):
                        broken.append({
                            "file": str(filepath),
                            "line": line_num,
                            "module": mod,
                            "symbol": None,
                            "full_import": mod,
                            "message": (
                                f"`{filepath}` imports `{mod}` "
                                f"but no matching file found"
                            ),
                        })

    return broken


def validate_changed_files(changed_files: list[str], base_dir: str | None = None) -> list[dict]:
    """
    Validate imports in changed files. Returns broken references.
    """
    from pathlib import Path

    base = base_dir or str(Path.cwd())
    all_broken = []

    for filepath in changed_files:
        path = Path(filepath)
        if path.suffix != ".py":
            continue
        if not path.is_file():
            continue
        broken = check_file_imports(str(path), base)
        all_broken.extend(broken)

    return all_broken


# â”€â”€ Stream Response â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def stream_response(messages: list[dict], config: dict) -> str:
    """Stream a response from Ollama with retry logic."""
    url = f"{config['ollama_url']}/api/chat"
    payload = {
        "model": config["model"],
        "messages": messages,
        "stream": True,
        "options": {
            "temperature": config.get("temperature", 0.7),
            "num_ctx": config.get("num_ctx", 32768),
            "num_predict": config.get("max_tokens", 4096),
        },
    }

    full_response = ""
    max_retries = 2
    tracker.start_request()

    for retry in range(max_retries + 1):
        try:
            if _show_streaming():
                with httpx.stream(
                    "POST", url, json=payload, timeout=120.0
                ) as resp:
                    resp.raise_for_status()
                    for line in resp.iter_lines():
                        if line:
                            data = json.loads(line)
                            chunk = data.get("message", {}).get("content", "")
                            if chunk:
                                full_response += chunk
                                tracker.count_token()
                                print(chunk, end="", flush=True)
                            if data.get("done"):
                                break
            else:
                with console.status(
                    "[bold cyan]Thinking[/bold cyan]",
                    spinner="dots12",
                    spinner_style="cyan",
                ) as status:
                    with httpx.stream(
                        "POST", url, json=payload, timeout=120.0
                    ) as resp:
                        resp.raise_for_status()
                        for line in resp.iter_lines():
                            if line:
                                data = json.loads(line)
                                chunk = data.get("message", {}).get("content", "")
                                if chunk:
                                    full_response += chunk
                                    tracker.count_token()
                                    word_count = len(full_response.split())
                                    status.update(
                                        f"[bold cyan]Thinking[/bold cyan] "
                                        f"[dim]({word_count} words)[/dim]"
                                    )
                                if data.get("done"):
                                    break
                if full_response.strip():
                    console.print(full_response)

            print()
            break

        except httpx.ConnectError:
            console.print(
                "\n[red]Error: Cannot connect to Ollama. "
                "Is it running?[/red]"
            )
            console.print("[dim]Start it with: ollama serve[/dim]")
            return ""

        except httpx.ReadTimeout:
            if retry < max_retries:
                console.print(
                    f"\n[yellow]âš  Timed out. "
                    f"Retrying ({retry + 1}/{max_retries})...[/yellow]"
                )
                if len(messages) > 5:
                    console.print(
                        "[dim]Trimming context for retry...[/dim]"
                    )
                    payload["messages"] = (
                        [messages[0]] + messages[-4:]
                    )
                full_response = ""
                continue
            console.print(
                "\n[red]Timed out after retries. "
                "Try /compact or a smaller model.[/red]"
            )
            return ""

        except httpx.RemoteProtocolError:
            console.print(
                "\n[red]Ollama disconnected â€” may be out of VRAM.[/red]"
            )
            console.print("[dim]Try: /model qwen2.5-coder:7b[/dim]")
            return ""

        except httpx.HTTPStatusError as e:
            console.print(
                f"\n[red]HTTP Error: {e.response.status_code}[/red]"
            )
            if e.response.status_code == 404:
                console.print(
                    f"[dim]Model '{config['model']}' not found. "
                    f"Try: /models to see available models[/dim]"
                )
            return ""

        except json.JSONDecodeError:
            console.print(
                "\n[red]Error: Invalid response from Ollama.[/red]"
            )
            console.print(
                "[dim]Ollama may be overloaded. Try again.[/dim]"
            )
            return ""

        except Exception as e:
            console.print(f"\n[red]Error: {e}[/red]")
            return ""

    if full_response and _show_metrics():
        prompt_tokens = estimate_message_tokens(messages)
        tracker.end_request(config["model"], prompt_tokens)

    return full_response


# â”€â”€ Read-Only Tool Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

READ_ONLY_TOOLS = frozenset({
    "read_file", "list_files", "list_tree",
    "find_files", "search_text", "grep",
    "file_info", "count_lines", "check_syntax",
    "check_port", "env_info", "fetch_url",
    "check_url", "list_deps", "git",
})

READ_ONLY_GIT = frozenset({
    "status", "log", "diff", "branch", "tag",
    "show", "remote", "stash list",
})


def is_tool_read_only(tool_name: str, tool_args: str = "") -> bool:
    """Check if a tool call is read-only (no side effects)."""
    if tool_name == "git":
        return any(
            tool_args.strip().startswith(cmd)
            for cmd in READ_ONLY_GIT
        )
    return tool_name in READ_ONLY_TOOLS


# â”€â”€ Chat Session â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ChatSession:
    def __init__(self, config: dict):
        self.config = config
        self._current_plan = None
        self._router = None
        self._undo = None
        self._last_review = None
        self._last_suggestions = None

        self.budget = ContextBudget(
            max_ctx=config.get("num_ctx", 32768),
            reserve_output=config.get("max_tokens", 4096),
        )

        self._warned_context = False
        self._hallucination_retries = 0
        self._max_hallucination_retries = 2

        # Load project memory
        memory_context = ""
        try:
            from memory import get_memory_context
            memory_context = get_memory_context()
            if memory_context:
                memory_context = f"\n\nProject Memory:\n{memory_context}"
        except Exception:
            pass

        self.messages = [
            {
                "role": "system",
                "content": (
                    config.get(
                        "system_prompt",
                        "You are a helpful AI coding assistant.",
                    )
                    + "\n"
                    + TOOL_DESCRIPTIONS
                    + f"\nWorking directory: {os.getcwd()}"
                    + memory_context
                ),
            }
        ]
        self.max_tool_iterations = 8

    def _manage_context(self):
        """Check context usage and auto-compact if needed."""
        usage = self.budget.usage(self.messages)

        if usage["status"] == "critical":
            console.print(
                "\n[red]âš  Context window nearly full! "
                "Auto-compacting...[/red]"
            )
            self.budget.display_bar(self.messages)
            self.messages = smart_compact(
                self.messages, self.config, self.budget, target_pct=0.4
            )
        elif usage["status"] == "compact":
            console.print(
                "\n[yellow]âš  Context getting large. "
                "Auto-compacting...[/yellow]"
            )
            self.budget.display_bar(self.messages)
            self.messages = smart_compact(
                self.messages, self.config, self.budget, target_pct=0.5
            )
        elif usage["status"] == "warning" and not self._warned_context:
            self.budget.display_bar(self.messages)
            console.print(
                "[dim]  Tip: Use /compact to free space[/dim]"
            )
            self._warned_context = True

    def _handle_hallucination(
        self, user_input: str, response: str
    ) -> bool:
        """Check for hallucinated file content and force tool use.

        Returns True if hallucination detected and correction injected.
        """
        is_file_hallucination = detect_hallucinated_files(
            user_input, response
        )
        is_content_hallucination = detect_hallucinated_content(
            user_input, response
        )

        if not is_file_hallucination and not is_content_hallucination:
            self._hallucination_retries = 0
            return False

        self._hallucination_retries += 1

        if self._hallucination_retries > self._max_hallucination_retries:
            console.print(
                "\n[yellow]âš  Model keeps generating fake output. "
                "Showing response as-is.[/yellow]"
            )
            console.print(
                "[dim]The output above may be fabricated. "
                "Use /scan or ask the model to use tools explicitly.[/dim]"
            )
            self._hallucination_retries = 0
            return False

        if is_file_hallucination:
            console.print(
                "\n[yellow]âš  Model generated fake file tree. "
                "Forcing tool use...[/yellow]"
            )
            correction = (
                "STOP. You just fabricated a file structure without "
                "reading the actual filesystem. This is WRONG. You MUST "
                "use the list_tree tool to see real files. "
                "Call the tool NOW with no other text:\n"
                "<tool:list_tree>.</tool>"
            )
        else:
            console.print(
                "\n[yellow]âš  Model generated fake file content. "
                "Forcing tool use...[/yellow]"
            )
            filename_match = re.search(
                r'\b([\w./\\-]+\.\w{1,5})\b', user_input
            )
            filename = (
                filename_match.group(1) if filename_match else "the file"
            )
            correction = (
                f"STOP. You just made up file contents without reading "
                f"the actual file. This is WRONG. You MUST use read_file "
                f"to see real contents. Call the tool NOW:\n"
                f"<tool:read_file>{filename}</tool>"
            )

        # Remove the hallucinated response from history
        if (
            self.messages
            and self.messages[-1]["role"] == "assistant"
            and self.messages[-1]["content"] == response
        ):
            self.messages.pop()

        # Inject correction as system guidance
        self.messages.append({
            "role": "system",
            "content": correction,
        })

        return True

    def _execute_tools(
        self, tool_calls: list[tuple[str, str]]
    ) -> tuple[str, bool, bool]:
        """Execute tool calls. Returns (result_text, has_read_only, has_write)."""
        tool_results = []
        has_read_only = False
        has_write = False

        for tool_name, tool_args in tool_calls:
            if tool_name in TOOL_MAP:
                if _show_tool_output():
                    console.print(
                        f"\n[bold green]âš¡ Tool: {tool_name}[/bold green]"
                    )
                    if tool_args and not tool_args.startswith("<<<"):
                        args_preview = tool_args[:100]
                        if len(tool_args) > 100:
                            args_preview += "..."
                        console.print(
                            f"[dim]  Args: {args_preview}[/dim]"
                        )

                try:
                    result = TOOL_MAP[tool_name](tool_args)
                except Exception as e:
                    result = f"Error executing {tool_name}: {e}"

                if _show_tool_output():
                    preview = result[:500]
                    if len(result) > 500:
                        preview += "..."
                    console.print(f"[dim]{preview}[/dim]")

                tool_results.append(
                    f"[Tool: {tool_name}] Result:\n{result}"
                )

                if is_tool_read_only(tool_name, tool_args):
                    has_read_only = True
                else:
                    has_write = True
            else:
                console.print(
                    f"\n[red]âš  Unknown tool: {tool_name}[/red]"
                )
                tool_results.append(
                    f"[Tool: {tool_name}] Error: Unknown tool. "
                    f"Available: {', '.join(sorted(TOOL_MAP.keys()))}"
                )

        result_text = "Tool results:\n\n" + "\n\n".join(tool_results)
        return result_text, has_read_only, has_write

    def _validate_written_files(self, tool_calls: list[tuple[str, str]]):
        """After write/edit tools, validate imports in changed files."""
        from pathlib import Path

        changed_files = []
        for tool_name, tool_args in tool_calls:
            if tool_name in ("write_file", "edit_file"):
                filepath = tool_args.split("\n", 1)[0].strip().strip("\"'`")
                if filepath and Path(filepath).suffix == ".py":
                    full_path = Path(filepath).resolve()
                    if full_path.is_file():
                        changed_files.append(str(full_path))

        if not changed_files:
            return

        broken = validate_file_references(changed_files)
        if broken:
            console.print(
                f"\n  [yellow]âœ— {len(broken)} broken reference(s)[/yellow]"
            )
            for ref in broken[:8]:
                console.print(f"    â€¢ {ref['message']}")
            if len(broken) > 8:
                console.print(
                    f"    [dim]... and {len(broken) - 8} more[/dim]"
                )

    def send(self, user_input: str) -> str:
        """Send user input, handle tool calls, return final response."""
        # Save undo state
        if self._undo is None:
            try:
                from undo import UndoManager
                self._undo = UndoManager()
            except ImportError:
                pass

        if self._undo:
            self._undo.save_state(
                self.messages,
                self.config.get("model", ""),
                "before send",
            )

        # Context management â€” before adding new message
        self._manage_context()

        # Route model if enabled
        if self._router and self._router.mode != "manual":
            self.config["model"] = self._router.route(user_input)

        self.messages.append({"role": "user", "content": user_input})

        # Tool loop
        last_tool_calls = ""
        repeated_count = 0
        response = ""

        for iteration in range(self.max_tool_iterations):
            console.print("\n[bold blue]Assistant:[/bold blue]")
            response = stream_response(self.messages, self.config)

            if not response:
                return ""

            self.messages.append({
                "role": "assistant",
                "content": response,
            })

            # Hallucination check (first iteration only)
            if iteration == 0 and self._handle_hallucination(
                user_input, response
            ):
                continue

            # Parse tool calls
            tool_calls = parse_tool_calls(response)

            if not tool_calls:
                break

            # Loop protection â€” detect repeated calls
            current_calls = str(tool_calls)
            if current_calls == last_tool_calls:
                repeated_count += 1
                if repeated_count >= 2:
                    console.print(
                        "\n[yellow]âš  Model repeating same tool call. "
                        "Stopping loop.[/yellow]"
                    )
                    break
            else:
                repeated_count = 0
            last_tool_calls = current_calls

            # Execute tools
            result_text, has_read_only, has_write = (
                self._execute_tools(tool_calls)
            )

            # Validate imports after file writes
            if has_write:
                self._validate_written_files(tool_calls)

            # â”€â”€ Smart error guidance injection â”€â”€
            # Diagnose errors and give the LLM specific, actionable
            # guidance instead of letting it guess randomly
            if _is_test_failure(result_text):
                error_guidance = format_error_guidance(result_text)
                result_text += error_guidance
                console.print(
                    "[dim]  ðŸ“‹ Error diagnosed â€” guidance injected[/dim]"
                )
            elif has_read_only and not has_write:
                result_text += (
                    "\n\nPresent these results clearly and concisely. "
                    "Do NOT execute follow-up actions unless asked. "
                    "Do NOT suggest changes unless asked."
                )

            # Tool results go as user role (Ollama doesn't support tool role)
            # but clearly marked as automated output
            self.messages.append({
                "role": "user",
                "content": (
                    "[SYSTEM: Tool execution results â€” "
                    "this is automated output, not user input]\n\n"
                    + result_text
                ),
            })

        self._hallucination_retries = 0
        self._show_context_usage()

        return response

    def _show_context_usage(self):
        """Show context bar if usage is getting high."""
        try:
            usage_after = self.budget.usage(self.messages)
            if usage_after["used_pct"] > 0.6:
                verbosity, Verbosity = _get_verbosity()
                if Verbosity is None or verbosity >= Verbosity.NORMAL:
                    self.budget.display_bar(self.messages)
        except Exception:
            pass

    def reset(self):
        """Clear conversation history."""
        self.messages = [self.messages[0]]
        self._warned_context = False
        self._hallucination_retries = 0
        console.print("[yellow]Conversation reset.[/yellow]")

    def compact(self):
        """Smart-compress conversation history."""
        self.budget.display_bar(self.messages)
        self.messages = smart_compact(
            self.messages, self.config, self.budget, target_pct=0.4
        )
        self.budget.display_bar(self.messages)
        self._warned_context = False

    def token_estimate(self) -> int:
        """Estimate total tokens in current context."""
        return estimate_message_tokens(self.messages)